{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f350c3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (630000, 13) test: (270000, 12)\n",
      "Initial train shape: (630000, 12) test shape: (270000, 12)\n",
      "Encoded feature dim: 49\n",
      "After stacking dim: 50\n",
      "[0]\tvalidation_0-rmse:18.41953\n",
      "[200]\tvalidation_0-rmse:8.77020\n",
      "[400]\tvalidation_0-rmse:8.72996\n",
      "[600]\tvalidation_0-rmse:8.70791\n",
      "[800]\tvalidation_0-rmse:8.69438\n",
      "[1000]\tvalidation_0-rmse:8.68662\n",
      "[1200]\tvalidation_0-rmse:8.68089\n",
      "[1400]\tvalidation_0-rmse:8.67752\n",
      "[1600]\tvalidation_0-rmse:8.67517\n",
      "[1800]\tvalidation_0-rmse:8.67272\n",
      "[2000]\tvalidation_0-rmse:8.67116\n",
      "[2200]\tvalidation_0-rmse:8.66995\n",
      "[2400]\tvalidation_0-rmse:8.66999\n",
      "[2472]\tvalidation_0-rmse:8.67037\n",
      "[Fold 1] best_iter=2273  RMSE=8.669538\n",
      "[0]\tvalidation_0-rmse:18.48868\n",
      "[200]\tvalidation_0-rmse:8.77458\n",
      "[400]\tvalidation_0-rmse:8.73446\n",
      "[600]\tvalidation_0-rmse:8.71117\n",
      "[800]\tvalidation_0-rmse:8.69851\n",
      "[1000]\tvalidation_0-rmse:8.68972\n",
      "[1200]\tvalidation_0-rmse:8.68409\n",
      "[1400]\tvalidation_0-rmse:8.68000\n",
      "[1600]\tvalidation_0-rmse:8.67716\n",
      "[1800]\tvalidation_0-rmse:8.67483\n",
      "[2000]\tvalidation_0-rmse:8.67219\n",
      "[2200]\tvalidation_0-rmse:8.67045\n",
      "[2400]\tvalidation_0-rmse:8.66977\n",
      "[2600]\tvalidation_0-rmse:8.66910\n",
      "[2697]\tvalidation_0-rmse:8.66916\n",
      "[Fold 2] best_iter=2498  RMSE=8.668923\n",
      "[0]\tvalidation_0-rmse:18.46776\n",
      "[200]\tvalidation_0-rmse:8.76928\n",
      "[400]\tvalidation_0-rmse:8.72708\n",
      "[600]\tvalidation_0-rmse:8.70439\n",
      "[800]\tvalidation_0-rmse:8.69060\n",
      "[1000]\tvalidation_0-rmse:8.68162\n",
      "[1200]\tvalidation_0-rmse:8.67685\n",
      "[1400]\tvalidation_0-rmse:8.67374\n",
      "[1600]\tvalidation_0-rmse:8.67099\n",
      "[1800]\tvalidation_0-rmse:8.66915\n",
      "[2000]\tvalidation_0-rmse:8.66839\n",
      "[2075]\tvalidation_0-rmse:8.66845\n",
      "[Fold 3] best_iter=1876  RMSE=8.668376\n",
      "[0]\tvalidation_0-rmse:18.52687\n",
      "[200]\tvalidation_0-rmse:8.78566\n",
      "[400]\tvalidation_0-rmse:8.74726\n",
      "[600]\tvalidation_0-rmse:8.72639\n",
      "[800]\tvalidation_0-rmse:8.71455\n",
      "[1000]\tvalidation_0-rmse:8.70759\n",
      "[1200]\tvalidation_0-rmse:8.70388\n",
      "[1400]\tvalidation_0-rmse:8.69980\n",
      "[1600]\tvalidation_0-rmse:8.69807\n",
      "[1800]\tvalidation_0-rmse:8.69723\n",
      "[2000]\tvalidation_0-rmse:8.69804\n",
      "[2070]\tvalidation_0-rmse:8.69827\n",
      "[Fold 4] best_iter=1871  RMSE=8.697159\n",
      "[0]\tvalidation_0-rmse:18.50467\n",
      "[200]\tvalidation_0-rmse:8.79739\n",
      "[400]\tvalidation_0-rmse:8.75897\n",
      "[600]\tvalidation_0-rmse:8.73656\n",
      "[800]\tvalidation_0-rmse:8.72250\n",
      "[1000]\tvalidation_0-rmse:8.71264\n",
      "[1200]\tvalidation_0-rmse:8.70780\n",
      "[1400]\tvalidation_0-rmse:8.70323\n",
      "[1600]\tvalidation_0-rmse:8.70000\n",
      "[1800]\tvalidation_0-rmse:8.69797\n",
      "[2000]\tvalidation_0-rmse:8.69636\n",
      "[2200]\tvalidation_0-rmse:8.69550\n",
      "[2400]\tvalidation_0-rmse:8.69488\n",
      "[2597]\tvalidation_0-rmse:8.69529\n",
      "[Fold 5] best_iter=2398  RMSE=8.694770\n",
      "\n",
      "=== XGB CV Summary ===\n",
      "OOF RMSE: 8.679763\n",
      "Best iters: [2273, 2498, 1876, 1871, 2398]\n",
      "Final n_estimators = 2183\n",
      "\n",
      "Wrote: /rds/rds-lxu/ml_datasets/exam_score_predict/submission.csv\n",
      "       id  exam_score\n",
      "0  630000   70.175308\n",
      "1  630001   69.450127\n",
      "2  630002   88.574509\n",
      "3  630003   58.442394\n",
      "4  630004   48.194519\n",
      "Submission columns: ['id', 'exam_score']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# XGBoost + Feature Engineering + Ridge Stacking (OOF feature)\n",
    "# For Kaggle \"Predicting Student Test Scores\" (PS S6E1)\n",
    "#\n",
    "# - Feature engineering:\n",
    "#   * high_study\n",
    "#   * study_att (interaction)\n",
    "#   * manual_formula (LUT-based linear score)\n",
    "#   * sin/cos transforms for study_hours + class_attendance (periods 12, 14)\n",
    "#   * a few safe numeric transforms (sqrt, square, ratios)\n",
    "#\n",
    "# - Stacking:\n",
    "#   * Ridge OOF predictions on engineered features -> feature_lr_pred\n",
    "#\n",
    "# - Model:\n",
    "#   * XGBoost GPU with early stopping inside folds\n",
    "#   * Final fit on full training using avg best_iteration\n",
    "#\n",
    "# Output:\n",
    "#   * prints CV RMSE\n",
    "#   * writes submission.csv (id, exam_score) in DATA_DIR\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "DATA_DIR = \"/rds/rds-lxu/ml_datasets/exam_score_predict\"\n",
    "TRAIN_PATH = f\"{DATA_DIR}/train.csv\"\n",
    "TEST_PATH  = f\"{DATA_DIR}/test.csv\"\n",
    "\n",
    "ID_COL = \"id\"\n",
    "TARGET = \"exam_score\"\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Load\n",
    "# -----------------------------\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df  = pd.read_csv(TEST_PATH)\n",
    "\n",
    "y = train_df[TARGET].astype(float).values\n",
    "train_X0 = train_df.drop(columns=[TARGET]).copy()\n",
    "test_X0  = test_df.copy()\n",
    "\n",
    "print(\"train:\", train_df.shape, \"test:\", test_df.shape)\n",
    "print(\"Initial train shape:\", train_X0.shape, \"test shape:\", test_X0.shape)\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Feature engineering\n",
    "# -----------------------------\n",
    "LUT = {\n",
    "    \"sleep_quality\": {\"good\": 5, \"average\": 0, \"poor\": -5},\n",
    "    \"facility_rating\": {\"high\": 4, \"medium\": 0, \"low\": -4},\n",
    "    \"study_method\": {\"coaching\": 10, \"mixed\": 5, \"group study\": 2, \"online videos\": 1, \"self-study\": 0},\n",
    "}\n",
    "\n",
    "def add_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # ensure consistent string normalization for LUT mapping\n",
    "    for c in [\"sleep_quality\", \"facility_rating\", \"study_method\", \"gender\", \"course\", \"internet_access\", \"exam_difficulty\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(str).str.strip()\n",
    "\n",
    "    # core engineered features\n",
    "    df[\"high_study\"] = (df[\"study_hours\"] >= 7.0).astype(int)\n",
    "    df[\"study_att\"] = df[\"study_hours\"] * df[\"class_attendance\"]\n",
    "\n",
    "    # safe numeric transforms\n",
    "    df[\"study_hours_sq\"] = df[\"study_hours\"] ** 2\n",
    "    df[\"class_attendance_sq\"] = df[\"class_attendance\"] ** 2\n",
    "    df[\"sleep_hours_sq\"] = df[\"sleep_hours\"] ** 2\n",
    "\n",
    "    df[\"study_hours_sqrt\"] = np.sqrt(np.clip(df[\"study_hours\"], 0.0, None))\n",
    "    df[\"sleep_hours_sqrt\"] = np.sqrt(np.clip(df[\"sleep_hours\"], 0.0, None))\n",
    "\n",
    "    df[\"att_per_hour\"] = df[\"class_attendance\"] / (df[\"study_hours\"] + 1e-3)\n",
    "    df[\"study_per_sleep\"] = df[\"study_hours\"] / (df[\"sleep_hours\"] + 1e-3)\n",
    "\n",
    "    # manual formula (as described in discussions)\n",
    "    sq = df[\"sleep_quality\"].map(LUT[\"sleep_quality\"]).fillna(0.0)\n",
    "    sm = df[\"study_method\"].map(LUT[\"study_method\"]).fillna(0.0)\n",
    "    fr = df[\"facility_rating\"].map(LUT[\"facility_rating\"]).fillna(0.0)\n",
    "\n",
    "    df[\"manual_formula\"] = (\n",
    "        6.0 * df[\"study_hours\"]\n",
    "        + 0.35 * df[\"class_attendance\"]\n",
    "        + 1.5 * df[\"sleep_hours\"]\n",
    "        + sq + sm + fr\n",
    "    )\n",
    "\n",
    "    # sinusoidal / cosine features (nonlinear transforms)\n",
    "    for p in [12, 14]:\n",
    "        df[f\"study_hours_sin_{p}\"] = np.sin(2 * np.pi * df[\"study_hours\"] / p)\n",
    "        df[f\"study_hours_cos_{p}\"] = np.cos(2 * np.pi * df[\"study_hours\"] / p)\n",
    "        df[f\"class_attendance_sin_{p}\"] = np.sin(2 * np.pi * df[\"class_attendance\"] / p)\n",
    "        df[f\"class_attendance_cos_{p}\"] = np.cos(2 * np.pi * df[\"class_attendance\"] / p)\n",
    "\n",
    "    return df\n",
    "\n",
    "train_X = add_features(train_X0)\n",
    "test_X  = add_features(test_X0)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) One-hot (train+test aligned)\n",
    "# -----------------------------\n",
    "# keep IDs separately\n",
    "train_id = train_X[ID_COL].values\n",
    "test_id  = test_X[ID_COL].values\n",
    "\n",
    "# combine then one-hot on object columns\n",
    "all_X = pd.concat([train_X, test_X], axis=0, ignore_index=True)\n",
    "\n",
    "obj_cols = all_X.select_dtypes(include=[\"object\"]).columns\n",
    "all_X[obj_cols] = all_X[obj_cols].astype(str).fillna(\"missing\")\n",
    "all_X = all_X.fillna(0.0)\n",
    "\n",
    "all_X_enc = pd.get_dummies(all_X, columns=obj_cols, drop_first=False)\n",
    "\n",
    "X_train = all_X_enc.iloc[:len(train_X)].values\n",
    "X_test  = all_X_enc.iloc[len(train_X):].values\n",
    "\n",
    "print(\"Encoded feature dim:\", X_train.shape[1])\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Ridge stacking feature: OOF predictions -> feature_lr_pred\n",
    "# -----------------------------\n",
    "N_SPLITS = 5\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "# RidgeCV picks alpha via inner CV; fast + strong for linear signal\n",
    "ridge = RidgeCV(alphas=np.logspace(-4, 4, 25), fit_intercept=True)\n",
    "\n",
    "oof_ridge = np.zeros(len(X_train), dtype=float)\n",
    "test_ridge_folds = np.zeros((N_SPLITS, len(X_test)), dtype=float)\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(kf.split(X_train), 1):\n",
    "    X_tr, y_tr = X_train[tr_idx], y[tr_idx]\n",
    "    X_va, y_va = X_train[va_idx], y[va_idx]\n",
    "\n",
    "    ridge.fit(X_tr, y_tr)\n",
    "    oof_ridge[va_idx] = ridge.predict(X_va)\n",
    "    test_ridge_folds[fold - 1] = ridge.predict(X_test)\n",
    "\n",
    "test_ridge = test_ridge_folds.mean(axis=0)\n",
    "\n",
    "# append stacking feature\n",
    "X_train_stack = np.hstack([X_train, oof_ridge.reshape(-1, 1)])\n",
    "X_test_stack  = np.hstack([X_test,  test_ridge.reshape(-1, 1)])\n",
    "\n",
    "print(\"After stacking dim:\", X_train_stack.shape[1])\n",
    "\n",
    "# -----------------------------\n",
    "# 5) XGBoost CV with early stopping\n",
    "# -----------------------------\n",
    "xgb_params = dict(\n",
    "    n_estimators=20000,           # large cap; early stop chooses effective trees\n",
    "    learning_rate=0.03,\n",
    "    max_depth=7,\n",
    "    min_child_weight=3.0,\n",
    "    subsample=0.85,\n",
    "    colsample_bytree=0.85,\n",
    "    reg_lambda=2.0,\n",
    "    reg_alpha=0.0,\n",
    "    gamma=0.0,\n",
    "    objective=\"reg:squarederror\",\n",
    "    eval_metric=\"rmse\",\n",
    "    tree_method=\"hist\",\n",
    "    device=\"cuda:0\",\n",
    "    early_stopping_rounds=200,    # must have eval_set per fold\n",
    "    verbosity=0,\n",
    ")\n",
    "\n",
    "oof_xgb = np.zeros(len(X_train_stack), dtype=float)\n",
    "test_pred_folds = np.zeros((N_SPLITS, len(X_test_stack)), dtype=float)\n",
    "best_iters = []\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(kf.split(X_train_stack), 1):\n",
    "    X_tr, y_tr = X_train_stack[tr_idx], y[tr_idx]\n",
    "    X_va, y_va = X_train_stack[va_idx], y[va_idx]\n",
    "\n",
    "    model = XGBRegressor(**xgb_params, random_state=42 + fold)\n",
    "    model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=200)\n",
    "\n",
    "    oof_xgb[va_idx] = model.predict(X_va)\n",
    "    test_pred_folds[fold - 1] = model.predict(X_test_stack)\n",
    "\n",
    "    best_iters.append(int(model.best_iteration) + 1)\n",
    "    rmse_fold = float(np.sqrt(mean_squared_error(y_va, oof_xgb[va_idx])))\n",
    "    print(f\"[Fold {fold}] best_iter={best_iters[-1]}  RMSE={rmse_fold:.6f}\")\n",
    "\n",
    "rmse_oof = float(np.sqrt(mean_squared_error(y, oof_xgb)))\n",
    "print(\"\\n=== XGB CV Summary ===\")\n",
    "print(f\"OOF RMSE: {rmse_oof:.6f}\")\n",
    "print(\"Best iters:\", best_iters)\n",
    "best_n = int(np.mean(best_iters))\n",
    "print(\"Final n_estimators =\", best_n)\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Final fit on full train (no early stopping) + predict test\n",
    "# -----------------------------\n",
    "final_params = xgb_params.copy()\n",
    "final_params.pop(\"early_stopping_rounds\", None)\n",
    "final_params[\"n_estimators\"] = best_n\n",
    "final_params[\"verbosity\"] = 1\n",
    "\n",
    "final_model = XGBRegressor(**final_params, random_state=123)\n",
    "final_model.fit(X_train_stack, y, verbose=False)\n",
    "\n",
    "test_pred = final_model.predict(X_test_stack)\n",
    "\n",
    "# clip to plausible range (optional, but often helps a bit)\n",
    "test_pred = np.clip(test_pred, 0.0, 100.0)\n",
    "\n",
    "# -----------------------------\n",
    "# 7) Submission\n",
    "# -----------------------------\n",
    "submission = pd.DataFrame({ID_COL: test_id, TARGET: test_pred})\n",
    "out_path = f\"{DATA_DIR}/submission.csv\"\n",
    "submission.to_csv(out_path, index=False)\n",
    "\n",
    "print(\"\\nWrote:\", out_path)\n",
    "print(submission.head())\n",
    "print(\"Submission columns:\", list(submission.columns))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cmbagent_env)",
   "language": "python",
   "name": "cmbagent_env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
