{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f350c3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "Categorical: 7, Numerical: 9\n",
      "Starting 3-Fold CV on Device: cuda\n",
      "\n",
      "--- Fold 1/3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/scratch-lxu/venv/cmbagent_env/lib/python3.12/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train MSE=4034.1796, Val RMSE=62.1150\n",
      "Epoch 5: Train MSE=80.1888, Val RMSE=8.9571\n",
      "Epoch 10: Train MSE=79.1263, Val RMSE=8.9743\n",
      "Early stopping triggered.\n",
      "Fold 1 Best RMSE: 8.85346\n",
      "\n",
      "--- Fold 2/3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/scratch-lxu/venv/cmbagent_env/lib/python3.12/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train MSE=3915.6929, Val RMSE=60.3187\n",
      "Epoch 5: Train MSE=80.1495, Val RMSE=9.0064\n",
      "Epoch 10: Train MSE=79.1396, Val RMSE=8.9473\n",
      "Early stopping triggered.\n",
      "Fold 2 Best RMSE: 8.89013\n",
      "\n",
      "--- Fold 3/3 ---\n",
      "Epoch 0: Train MSE=3955.8551, Val RMSE=60.7932\n",
      "Epoch 5: Train MSE=79.7508, Val RMSE=8.8974\n",
      "Epoch 10: Train MSE=78.9539, Val RMSE=8.9316\n",
      "Early stopping triggered.\n",
      "Fold 3 Best RMSE: 8.88403\n",
      "\n",
      ">>> Transformer CV RMSE: 8.91703\n",
      "Saved Transformer submission to /rds/rds-lxu/ml_datasets/exam_score_predict/submission_transformer.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Tabular Transformer for Exam Score Prediction\n",
    "# Architecture: Feature Tokenizer + Transformer Encoder\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "DATA_DIR = \"/rds/rds-lxu/ml_datasets/exam_score_predict\"\n",
    "TRAIN_PATH = f\"{DATA_DIR}/train.csv\"\n",
    "TEST_PATH  = f\"{DATA_DIR}/test.csv\"\n",
    "SUB_PATH   = f\"{DATA_DIR}/submission_transformer.csv\"\n",
    "\n",
    "# Hyperparameters\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED = 42\n",
    "N_SPLITS = 3\n",
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "PATIENCE = 5        # Early stopping rounds\n",
    "\n",
    "# Model Config\n",
    "D_MODEL = 64        # Embedding dimension\n",
    "N_HEADS = 4         # Attention heads\n",
    "N_LAYERS = 3        # Transformer blocks\n",
    "DROPOUT = 0.1\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Data Loading & Feature Engineering\n",
    "# -----------------------------\n",
    "print(\"Loading Data...\")\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df  = pd.read_csv(TEST_PATH)\n",
    "\n",
    "# (Same Feature Engineering as before - Crucial for consistency)\n",
    "LUT = {\n",
    "    \"sleep_quality\": {\"good\": 5, \"average\": 0, \"poor\": -5},\n",
    "    \"facility_rating\": {\"high\": 4, \"medium\": 0, \"low\": -4},\n",
    "    \"study_method\": {\"coaching\": 10, \"mixed\": 5, \"group study\": 2, \"online videos\": 1, \"self-study\": 0},\n",
    "}\n",
    "\n",
    "def preprocess_df(df):\n",
    "    df = df.copy()\n",
    "    # String normalization\n",
    "    cat_cols = [\"sleep_quality\", \"facility_rating\", \"study_method\", \"gender\", \n",
    "                \"course\", \"internet_access\", \"exam_difficulty\"]\n",
    "    for c in cat_cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(str).str.strip()\n",
    "\n",
    "    # Manual Scoring\n",
    "    sq = df[\"sleep_quality\"].map(LUT[\"sleep_quality\"]).fillna(0.0)\n",
    "    sm = df[\"study_method\"].map(LUT[\"study_method\"]).fillna(0.0)\n",
    "    fr = df[\"facility_rating\"].map(LUT[\"facility_rating\"]).fillna(0.0)\n",
    "\n",
    "    df[\"manual_formula\"] = (\n",
    "        6.0 * df[\"study_hours\"]\n",
    "        + 0.35 * df[\"class_attendance\"]\n",
    "        + 1.5 * df[\"sleep_hours\"]\n",
    "        + sq + sm + fr\n",
    "    )\n",
    "    \n",
    "    # Interactions\n",
    "    df[\"study_att\"] = df[\"study_hours\"] * df[\"class_attendance\"]\n",
    "    df[\"study_div_sleep\"] = df[\"study_hours\"] / (df[\"sleep_hours\"] + 1e-4)\n",
    "    \n",
    "    # Trig\n",
    "    for p in [12, 14]:\n",
    "        df[f\"sin_study_{p}\"] = np.sin(2 * np.pi * df[\"study_hours\"] / p)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = preprocess_df(train_df)\n",
    "test_df  = preprocess_df(test_df)\n",
    "\n",
    "target = train_df[\"exam_score\"].values.astype(np.float32)\n",
    "train_df = train_df.drop(columns=[\"exam_score\", \"id\"])\n",
    "test_ids = test_df[\"id\"].values\n",
    "test_df  = test_df.drop(columns=[\"id\"])\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Preprocessing for Neural Net\n",
    "# -----------------------------\n",
    "# Identify columns\n",
    "cat_cols = [c for c in train_df.columns if train_df[c].dtype == 'object']\n",
    "num_cols = [c for c in train_df.columns if c not in cat_cols]\n",
    "\n",
    "print(f\"Categorical: {len(cat_cols)}, Numerical: {len(num_cols)}\")\n",
    "\n",
    "# Fill NaNs (Neural Nets hate NaNs)\n",
    "for c in num_cols:\n",
    "    train_df[c] = train_df[c].fillna(train_df[c].mean())\n",
    "    test_df[c] = test_df[c].fillna(train_df[c].mean())\n",
    "\n",
    "for c in cat_cols:\n",
    "    train_df[c] = train_df[c].fillna(\"MISSING\")\n",
    "    test_df[c]  = test_df[c].fillna(\"MISSING\")\n",
    "\n",
    "# Scale Numerics (StandardScaler is vital for convergence)\n",
    "scaler = StandardScaler()\n",
    "train_df[num_cols] = scaler.fit_transform(train_df[num_cols])\n",
    "test_df[num_cols]  = scaler.transform(test_df[num_cols])\n",
    "\n",
    "# Encode Categoricals (LabelEncoder for Embeddings)\n",
    "# We map each unique category to an integer id: 0, 1, 2...\n",
    "cat_cardinalities = []\n",
    "for c in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    # Fit on both to ensure we cover all categories\n",
    "    full_list = pd.concat([train_df[c], test_df[c]], axis=0).astype(str)\n",
    "    le.fit(full_list)\n",
    "    train_df[c] = le.transform(train_df[c].astype(str))\n",
    "    test_df[c]  = le.transform(test_df[c].astype(str))\n",
    "    cat_cardinalities.append(len(le.classes_))\n",
    "\n",
    "X_train_full = train_df.values.astype(np.float32)\n",
    "X_test_full  = test_df.values.astype(np.float32)\n",
    "\n",
    "# Indices for the model to know which columns are which\n",
    "cat_idxs = [train_df.columns.get_loc(c) for c in cat_cols]\n",
    "num_idxs = [train_df.columns.get_loc(c) for c in num_cols]\n",
    "\n",
    "# -----------------------------\n",
    "# 3) PyTorch Dataset\n",
    "# -----------------------------\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Return features and target\n",
    "        if self.y is not None:\n",
    "            return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.float32)\n",
    "        else:\n",
    "            return torch.tensor(self.X[idx], dtype=torch.float32)\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Transformer Model\n",
    "# -----------------------------\n",
    "class ExamTransformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_cols_idx, \n",
    "                 cat_cols_idx, \n",
    "                 cat_counts, \n",
    "                 d_model=64, \n",
    "                 n_heads=4, \n",
    "                 n_layers=3, \n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_idx = num_cols_idx\n",
    "        self.cat_idx = cat_cols_idx\n",
    "        \n",
    "        # --- Feature Tokenizer ---\n",
    "        # 1. Numerical Embedding: Project scalar -> vector\n",
    "        self.num_embeddings = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(1, d_model),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(d_model, d_model)\n",
    "            ) for _ in num_cols_idx\n",
    "        ])\n",
    "        \n",
    "        # 2. Categorical Embedding: Integer -> vector\n",
    "        self.cat_embeddings = nn.ModuleList([\n",
    "            nn.Embedding(count, d_model) for count in cat_counts\n",
    "        ])\n",
    "        \n",
    "        # [CLS] Token to aggregate info\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        \n",
    "        # --- Transformer Encoder ---\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=n_heads, \n",
    "            dim_feedforward=d_model*4, \n",
    "            dropout=dropout, \n",
    "            batch_first=True,\n",
    "            norm_first=True  # Pre-Norm is often more stable\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        # --- Head ---\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Process Numerical features\n",
    "        # x shape: [Batch, Total_Features]\n",
    "        num_inputs = [x[:, i].unsqueeze(-1) for i in self.num_idx] # List of [Batch, 1]\n",
    "        num_embeds = [emb(v) for emb, v in zip(self.num_embeddings, num_inputs)] # List of [Batch, D]\n",
    "        num_embeds = torch.stack(num_embeds, dim=1) # [Batch, N_num, D]\n",
    "        \n",
    "        # Process Categorical features\n",
    "        cat_inputs = [x[:, i].long() for i in self.cat_idx] # List of [Batch]\n",
    "        cat_embeds = [emb(v) for emb, v in zip(self.cat_embeddings, cat_inputs)] # List of [Batch, D]\n",
    "        cat_embeds = torch.stack(cat_embeds, dim=1) # [Batch, N_cat, D]\n",
    "        \n",
    "        # Concatenate all features -> Sequence\n",
    "        # Sequence: [CLS] + [Num Features] + [Cat Features]\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        \n",
    "        # Combine\n",
    "        sequence = torch.cat([cls_tokens, num_embeds, cat_embeds], dim=1) \n",
    "        \n",
    "        # Transformer Pass\n",
    "        output = self.transformer(sequence)\n",
    "        \n",
    "        # Take the [CLS] token output (index 0)\n",
    "        cls_output = output[:, 0, :]\n",
    "        \n",
    "        # Final prediction\n",
    "        pred = self.head(cls_output)\n",
    "        return pred.squeeze(-1)\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Training Loop\n",
    "# -----------------------------\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "oof_preds = np.zeros(len(train_df))\n",
    "test_preds_folds = np.zeros((N_SPLITS, len(test_df)))\n",
    "\n",
    "print(f\"Starting {N_SPLITS}-Fold CV on Device: {DEVICE}\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_full)):\n",
    "    print(f\"\\n--- Fold {fold+1}/{N_SPLITS} ---\")\n",
    "    \n",
    "    # Prepare DataLoaders\n",
    "    train_dataset = TabularDataset(X_train_full[train_idx], target[train_idx])\n",
    "    val_dataset   = TabularDataset(X_train_full[val_idx], target[val_idx])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE*2, shuffle=False)\n",
    "    \n",
    "    # Initialize Model\n",
    "    model = ExamTransformer(\n",
    "        num_cols_idx=num_idxs,\n",
    "        cat_cols_idx=cat_idxs,\n",
    "        cat_counts=cat_cardinalities,\n",
    "        d_model=D_MODEL,\n",
    "        n_heads=N_HEADS,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=DROPOUT\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=LEARNING_RATE, steps_per_epoch=len(train_loader), epochs=EPOCHS\n",
    "    )\n",
    "    \n",
    "    # Training Loop\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            preds = model(X_batch)\n",
    "            loss = criterion(preds, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "                preds = model(X_batch)\n",
    "                loss = criterion(preds, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_rmse = np.sqrt(val_loss)\n",
    "        \n",
    "        # Simple logging\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"Epoch {epoch}: Train MSE={train_loss:.4f}, Val RMSE={val_rmse:.4f}\")\n",
    "            \n",
    "        # Early Stopping Check\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "    \n",
    "    # Load best model for inference\n",
    "    model.load_state_dict(best_model_state)\n",
    "    model.eval()\n",
    "    \n",
    "    # Predict OOF\n",
    "    val_preds = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch = X_batch.to(DEVICE)\n",
    "            preds = model(X_batch)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "    oof_preds[val_idx] = np.array(val_preds)\n",
    "    \n",
    "    # Predict Test\n",
    "    test_dataset = TabularDataset(X_test_full)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE*2, shuffle=False)\n",
    "    \n",
    "    fold_test_preds = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch in test_loader:\n",
    "            X_batch = X_batch.to(DEVICE)\n",
    "            preds = model(X_batch)\n",
    "            fold_test_preds.extend(preds.cpu().numpy())\n",
    "    test_preds_folds[fold] = np.array(fold_test_preds)\n",
    "    \n",
    "    print(f\"Fold {fold+1} Best RMSE: {np.sqrt(best_loss):.5f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Final Submission\n",
    "# -----------------------------\n",
    "final_rmse = np.sqrt(mean_squared_error(target, oof_preds))\n",
    "print(f\"\\n>>> Transformer CV RMSE: {final_rmse:.5f}\")\n",
    "\n",
    "avg_test_pred = test_preds_folds.mean(axis=0)\n",
    "avg_test_pred = np.clip(avg_test_pred, 0, 100)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": test_ids,\n",
    "    \"exam_score\": avg_test_pred\n",
    "})\n",
    "submission.to_csv(SUB_PATH, index=False)\n",
    "print(f\"Saved Transformer submission to {SUB_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aa1564d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing Matrices...\n",
      "Matrix Shape: (630000, 18)\n",
      "Calculating analytic solution...\n",
      "Solved successfully.\n",
      "Submission saved to /rds/rds-lxu/ml_datasets/exam_score_predict/submission_analytic.csv\n",
      "       id  exam_score\n",
      "0  630000   71.705458\n",
      "1  630001   69.715685\n",
      "2  630002   86.974607\n",
      "3  630003   54.442378\n",
      "4  630004   47.074184\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Analytic Solution: Polynomial Ridge Regression\n",
    "# Method: Normal Equation with Regularization (Closed-Form)\n",
    "# \"No Training\" loops - just one Matrix Calculation.\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.linalg as la  # Pure Linear Algebra\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "DATA_DIR = \"/rds/rds-lxu/ml_datasets/exam_score_predict\"\n",
    "TRAIN_PATH = f\"{DATA_DIR}/train.csv\"\n",
    "TEST_PATH  = f\"{DATA_DIR}/test.csv\"\n",
    "SUB_PATH   = f\"{DATA_DIR}/submission_analytic.csv\"\n",
    "\n",
    "# Regularization Strength (Lambda)\n",
    "# Prevents \"Singular Matrix\" crashes. \n",
    "# Higher = simpler model, Lower = complex fit.\n",
    "LAMBDA = 15.0 \n",
    "\n",
    "# -----------------------------\n",
    "# 1) Robust Data Loading\n",
    "# -----------------------------\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df  = pd.read_csv(TEST_PATH)\n",
    "\n",
    "y_train = train_df[\"exam_score\"].values\n",
    "test_ids = test_df[\"id\"].values\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Manual Feature Engineering\n",
    "# -----------------------------\n",
    "# We map everything to numbers. No \"One-Hot\" expansion to save memory.\n",
    "LUT = {\n",
    "    \"sleep_quality\": {\"good\": 1, \"average\": 0, \"poor\": -1},\n",
    "    \"facility_rating\": {\"high\": 1, \"medium\": 0, \"low\": -1},\n",
    "    \"study_method\": {\"coaching\": 2, \"mixed\": 1, \"group study\": 0.5, \"online videos\": 0.5, \"self-study\": 0},\n",
    "    \"exam_difficulty\": {\"hard\": -1, \"medium\": 0, \"easy\": 1},\n",
    "    \"internet_access\": {\"yes\": 1, \"no\": 0},\n",
    "    \"gender\": {\"Male\": 0, \"Female\": 1} \n",
    "}\n",
    "\n",
    "def get_feature_matrix(df):\n",
    "    # Make a copy to avoid warnings\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Map Categoricals to Ordinals\n",
    "    for col, mapping in LUT.items():\n",
    "        if col in df.columns:\n",
    "            # Map knowns, fill unknowns with 0 (neutral)\n",
    "            df[col] = df[col].map(mapping).fillna(0)\n",
    "\n",
    "    # 2. Select Base Features (Numerical + Mapped)\n",
    "    features = [\n",
    "        \"study_hours\", \"class_attendance\", \"sleep_hours\", \n",
    "        \"previous_exam_score\", \"tutoring_sessions\", \"physical_activity\",\n",
    "        \"sleep_quality\", \"facility_rating\", \"study_method\", \n",
    "        \"exam_difficulty\", \"internet_access\"\n",
    "    ]\n",
    "    # Filter for columns that actually exist\n",
    "    use_cols = [c for c in features if c in df.columns]\n",
    "    \n",
    "    # 3. Extract Matrix\n",
    "    X = df[use_cols].values.astype(np.float64)\n",
    "    \n",
    "    # 4. Fill NaNs (Mean Imputation)\n",
    "    # Fast numpy way to fill NaNs with column means\n",
    "    col_mean = np.nanmean(X, axis=0)\n",
    "    inds = np.where(np.isnan(X))\n",
    "    X[inds] = np.take(col_mean, inds[1])\n",
    "    \n",
    "    # 5. Polynomial Expansion (The \"Optimization\")\n",
    "    # We add Squared terms to capture non-linearities (Curves)\n",
    "    # e.g., \"study_hours^2\" helps model burnout\n",
    "    X_poly = X ** 2\n",
    "    \n",
    "    # 6. Interaction Terms (Manual)\n",
    "    # study * attendance is often powerful\n",
    "    if \"study_hours\" in df.columns and \"class_attendance\" in df.columns:\n",
    "        interaction = (df[\"study_hours\"] * df[\"class_attendance\"]).values.reshape(-1, 1)\n",
    "        X = np.hstack([X, X_poly, interaction])\n",
    "    else:\n",
    "        X = np.hstack([X, X_poly])\n",
    "        \n",
    "    # 7. Add Bias Term (Intercept) column of 1s\n",
    "    ones = np.ones((X.shape[0], 1))\n",
    "    X = np.hstack([ones, X])\n",
    "    \n",
    "    return X\n",
    "\n",
    "print(\"Constructing Matrices...\")\n",
    "X_train = get_feature_matrix(train_df)\n",
    "X_test  = get_feature_matrix(test_df)\n",
    "\n",
    "print(f\"Matrix Shape: {X_train.shape}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3) The Solution (Normal Equation)\n",
    "# -----------------------------\n",
    "# We solve: (X.T @ X + lambda*I) * w = X.T @ y\n",
    "\n",
    "print(\"Calculating analytic solution...\")\n",
    "\n",
    "# A = X^T * X\n",
    "A = X_train.T @ X_train\n",
    "\n",
    "# Add Regularization (Ridge) to the diagonal\n",
    "# This makes the matrix invertible (Prevents Crash)\n",
    "I = np.eye(A.shape[0])\n",
    "A_ridge = A + (LAMBDA * I)\n",
    "\n",
    "# b = X^T * y\n",
    "b = X_train.T @ y_train\n",
    "\n",
    "# Solve for weights w using Cholesky solve (Faster/Stable than inversion)\n",
    "# Aw = b  =>  w = solve(A, b)\n",
    "try:\n",
    "    w = la.solve(A_ridge, b)\n",
    "    print(\"Solved successfully.\")\n",
    "except la.LinAlgError:\n",
    "    # Fallback to least squares if singular (unlikely with Lambda)\n",
    "    w = la.lstsq(A_ridge, b)[0]\n",
    "    print(\"Solved using least squares approximation.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Prediction\n",
    "# -----------------------------\n",
    "# y_pred = X_test @ w\n",
    "predictions = X_test @ w\n",
    "\n",
    "# Clip reasonable range (0-100)\n",
    "predictions = np.clip(predictions, 0, 100)\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Submission\n",
    "# -----------------------------\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": test_ids,\n",
    "    \"exam_score\": predictions\n",
    "})\n",
    "\n",
    "submission.to_csv(SUB_PATH, index=False)\n",
    "print(f\"Submission saved to {SUB_PATH}\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91f68b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cmbagent_env)",
   "language": "python",
   "name": "cmbagent_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
