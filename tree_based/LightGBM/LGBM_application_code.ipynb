{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8ef4800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Data ---\n",
      "Train Shape: (650000, 11)\n",
      "--- Processing Features ---\n",
      "\n",
      "--- Generating Ridge Meta-Feature (Numeric Only) ---\n",
      "Final Data Shape: (650000, 19)\n",
      "\n",
      "--- Training LightGBM ---\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 1, number of used features: 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA RTX PRO 6000 Blackwell Workstation Edition, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
      "[LightGBM] [Info] Start training from score 1.000000\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "GPU Detected and Enabled.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's rmse: 8.78803\n",
      "[2000]\tvalid_0's rmse: 8.76142\n",
      "[3000]\tvalid_0's rmse: 8.7475\n",
      "[4000]\tvalid_0's rmse: 8.74057\n",
      "[5000]\tvalid_0's rmse: 8.73669\n",
      "[6000]\tvalid_0's rmse: 8.73434\n",
      "[7000]\tvalid_0's rmse: 8.73258\n",
      "Early stopping, best iteration is:\n",
      "[7564]\tvalid_0's rmse: 8.73149\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's rmse: 8.84038\n",
      "[2000]\tvalid_0's rmse: 8.81213\n",
      "[3000]\tvalid_0's rmse: 8.80114\n",
      "[4000]\tvalid_0's rmse: 8.79424\n",
      "[5000]\tvalid_0's rmse: 8.78956\n",
      "[6000]\tvalid_0's rmse: 8.78512\n",
      "[7000]\tvalid_0's rmse: 8.78245\n",
      "[8000]\tvalid_0's rmse: 8.77927\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[7998]\tvalid_0's rmse: 8.77926\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's rmse: 8.78539\n",
      "[2000]\tvalid_0's rmse: 8.75846\n",
      "[3000]\tvalid_0's rmse: 8.74759\n",
      "[4000]\tvalid_0's rmse: 8.74\n",
      "[5000]\tvalid_0's rmse: 8.73481\n",
      "[6000]\tvalid_0's rmse: 8.73203\n",
      "Early stopping, best iteration is:\n",
      "[6565]\tvalid_0's rmse: 8.73131\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's rmse: 8.8202\n",
      "[2000]\tvalid_0's rmse: 8.79211\n",
      "[3000]\tvalid_0's rmse: 8.77891\n",
      "[4000]\tvalid_0's rmse: 8.77057\n",
      "[5000]\tvalid_0's rmse: 8.76513\n",
      "[6000]\tvalid_0's rmse: 8.76202\n",
      "[7000]\tvalid_0's rmse: 8.75955\n",
      "[8000]\tvalid_0's rmse: 8.75795\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[7998]\tvalid_0's rmse: 8.75794\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's rmse: 8.84831\n",
      "[2000]\tvalid_0's rmse: 8.82512\n",
      "[3000]\tvalid_0's rmse: 8.81293\n",
      "[4000]\tvalid_0's rmse: 8.80691\n",
      "[5000]\tvalid_0's rmse: 8.80286\n",
      "[6000]\tvalid_0's rmse: 8.8004\n",
      "[7000]\tvalid_0's rmse: 8.7975\n",
      "Early stopping, best iteration is:\n",
      "[7641]\tvalid_0's rmse: 8.79635\n",
      "\n",
      "Overall CV RMSE: 8.75931\n",
      "Saved submission to '/rds/rds-lxu/ml_datasets/exam_score_predict/submission_lgb_refined.csv'\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# 1. IMPORTS & CONFIGURATION\n",
    "# ==========================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "import gc\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "RANDOM_STATE = 42\n",
    "N_SPLITS = 5 \n",
    "\n",
    "# --- FIXED PARAMETERS TO PREVENT CPU EXPLOSION ---\n",
    "LGB_PARAMS = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'n_estimators': 8000,\n",
    "    'learning_rate': 0.015,\n",
    "    'num_leaves': 31,\n",
    "    'max_depth': 8,\n",
    "    'min_child_samples': 50,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1,\n",
    "    \n",
    "    # CRITICAL FIXES\n",
    "    'n_jobs': 8,               # <--- CHANGED from -1 to 8 to prevent thread explosion\n",
    "    'device': 'gpu',           # Try to use GPU\n",
    "    'gpu_platform_id': 0,\n",
    "    'gpu_device_id': 0,\n",
    "    \n",
    "    'random_state': RANDOM_STATE,\n",
    "    'verbosity': -1\n",
    "}\n",
    "\n",
    "# ==========================================================\n",
    "# 2. DATA LOADING & MEMORY OPTIMIZATION\n",
    "# ==========================================================\n",
    "print(\"--- Loading Data ---\")\n",
    "DATA_DIR = \"/rds/rds-lxu/ml_datasets/exam_score_predict\"\n",
    "\n",
    "# Load columns\n",
    "df_train = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "df_test = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "original = pd.read_csv(f'{DATA_DIR}/Exam_Score_Prediction.csv')\n",
    "\n",
    "# Handle ID columns\n",
    "df_train = df_train.set_index('id')\n",
    "df_test = df_test.set_index('id')\n",
    "if 'student_id' in original.columns:\n",
    "    original = original.set_index('student_id')\n",
    "\n",
    "# Merge\n",
    "df_train = pd.concat([original, df_train], axis=0).reset_index(drop=True)\n",
    "\n",
    "# Separate Target\n",
    "y = df_train['exam_score'].values.astype(np.float32)\n",
    "df_train = df_train.drop(columns=['exam_score'])\n",
    "\n",
    "print(f\"Train Shape: {df_train.shape}\")\n",
    "gc.collect()\n",
    "\n",
    "# ==========================================================\n",
    "# 3. FEATURE ENGINEERING\n",
    "# ==========================================================\n",
    "def preprocess(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Ordinal Mapping\n",
    "    mappings = {\n",
    "        'sleep_quality': {'poor': 1, 'average': 2, 'good': 3},\n",
    "        'facility_rating': {'low': 1, 'medium': 2, 'high': 3},\n",
    "        'exam_difficulty': {'easy': 1, 'moderate': 2, 'hard': 3}\n",
    "    }\n",
    "    for col, mapping in mappings.items():\n",
    "        if col in df.columns:\n",
    "            df[f'{col}_score'] = df[col].map(mapping).fillna(2).astype(np.float32)\n",
    "\n",
    "    # 2. Interactions\n",
    "    if 'study_hours' in df.columns and 'sleep_quality_score' in df.columns:\n",
    "        df['study_efficiency'] = df['study_hours'] * df['sleep_quality_score']\n",
    "    if 'class_attendance' in df.columns and 'facility_rating_score' in df.columns:\n",
    "        df['attendance_impact'] = df['class_attendance'] * df['facility_rating_score']\n",
    "    if 'study_hours' in df.columns:\n",
    "        df['study_hours_sq'] = df['study_hours'] ** 2\n",
    "\n",
    "    # 3. Formula Feature\n",
    "    is_good = (df['sleep_quality'] == 'good').astype(np.float32)\n",
    "    is_poor = (df['sleep_quality'] == 'poor').astype(np.float32)\n",
    "    is_coaching = (df['study_method'] == 'coaching').astype(np.float32)\n",
    "    is_high_fac = (df['facility_rating'] == 'high').astype(np.float32)\n",
    "    \n",
    "    df['formula'] = (6 * df['study_hours'] + \n",
    "                     0.35 * df['class_attendance'] + \n",
    "                     1.5 * df['sleep_hours'] +\n",
    "                     5 * is_good - 5 * is_poor +\n",
    "                     10 * is_coaching + \n",
    "                     4 * is_high_fac)\n",
    "\n",
    "    # 4. Convert Categoricals to 'category' dtype for LightGBM\n",
    "    cat_cols = ['gender', 'course', 'study_method', 'internet_access', \n",
    "                'sleep_quality', 'facility_rating', 'exam_difficulty']\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype('category')\n",
    "            \n",
    "    return df\n",
    "\n",
    "print(\"--- Processing Features ---\")\n",
    "X = preprocess(df_train)\n",
    "X_test = preprocess(df_test)\n",
    "\n",
    "# Identify Numeric Columns for Ridge (Skip categories to save RAM)\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Optimize Memory: Downcast floats\n",
    "for col in num_cols:\n",
    "    X[col] = X[col].astype(np.float32)\n",
    "    X_test[col] = X_test[col].astype(np.float32)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# ==========================================================\n",
    "# 4. LIGHTWEIGHT RIDGE STACKING (Numeric Only)\n",
    "# ==========================================================\n",
    "print(\"\\n--- Generating Ridge Meta-Feature (Numeric Only) ---\")\n",
    "\n",
    "# Scale numerics for Ridge\n",
    "scaler = StandardScaler()\n",
    "X_num_scaled = scaler.fit_transform(X[num_cols].fillna(0))\n",
    "X_test_num_scaled = scaler.transform(X_test[num_cols].fillna(0))\n",
    "\n",
    "kf_ridge = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "ridge_oof = np.zeros((X.shape[0],), dtype=np.float32)\n",
    "ridge_test_preds = np.zeros((X_test.shape[0], 5), dtype=np.float32)\n",
    "\n",
    "model_ridge = RidgeCV(alphas=[0.1, 1.0, 10.0], scoring='neg_root_mean_squared_error')\n",
    "\n",
    "for fold, (idx_tr, idx_va) in enumerate(kf_ridge.split(X_num_scaled, y)):\n",
    "    model_ridge.fit(X_num_scaled[idx_tr], y[idx_tr])\n",
    "    ridge_oof[idx_va] = model_ridge.predict(X_num_scaled[idx_va])\n",
    "    ridge_test_preds[:, fold] = model_ridge.predict(X_test_num_scaled)\n",
    "\n",
    "# Add Ridge Prediction as Feature\n",
    "X['ridge_pred'] = ridge_oof\n",
    "X_test['ridge_pred'] = ridge_test_preds.mean(axis=1)\n",
    "\n",
    "# Clean up to save RAM\n",
    "del X_num_scaled, X_test_num_scaled, ridge_test_preds\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Final Data Shape: {X.shape}\")\n",
    "\n",
    "# ==========================================================\n",
    "# 5. LIGHTGBM TRAINING (GPU)\n",
    "# ==========================================================\n",
    "print(\"\\n--- Training LightGBM ---\")\n",
    "# Check if GPU is actually being used by creating a small test model\n",
    "try:\n",
    "    lgb.train({'device': 'gpu'}, lgb.Dataset(np.array([[1]]), np.array([1])), num_boost_round=1)\n",
    "    print(\"GPU Detected and Enabled.\")\n",
    "except Exception as e:\n",
    "    print(\"WARNING: GPU init failed. Falling back to CPU with n_jobs=8.\")\n",
    "    print(f\"Error: {e}\")\n",
    "    LGB_PARAMS['device'] = 'cpu'\n",
    "\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "lgb_oof_preds = np.zeros(X.shape[0])\n",
    "lgb_test_preds = np.zeros((X_test.shape[0], N_SPLITS))\n",
    "\n",
    "for fold, (idx_tr, idx_va) in enumerate(kf.split(X, y)):\n",
    "    X_tr, y_tr = X.iloc[idx_tr], y[idx_tr]\n",
    "    X_va, y_va = X.iloc[idx_va], y[idx_va]\n",
    "    \n",
    "    model = lgb.LGBMRegressor(**LGB_PARAMS)\n",
    "    \n",
    "    callbacks = [\n",
    "        lgb.early_stopping(stopping_rounds=100, verbose=True),\n",
    "        lgb.log_evaluation(period=1000)\n",
    "    ]\n",
    "    \n",
    "    model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_va, y_va)],\n",
    "        eval_metric='rmse',\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    lgb_oof_preds[idx_va] = model.predict(X_va)\n",
    "    lgb_test_preds[:, fold] = model.predict(X_test)\n",
    "    \n",
    "    gc.collect() \n",
    "\n",
    "# ==========================================================\n",
    "# 6. SUBMISSION\n",
    "# ==========================================================\n",
    "final_rmse = np.sqrt(mean_squared_error(y, lgb_oof_preds))\n",
    "print(f\"\\nOverall CV RMSE: {final_rmse:.5f}\")\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': df_test.index,\n",
    "    'exam_score': np.clip(lgb_test_preds.mean(axis=1), 0, 100)\n",
    "})\n",
    "\n",
    "submission.to_csv(f'{DATA_DIR}/submission_lgb_refined.csv', index=False)\n",
    "print(f\"Saved submission to '{DATA_DIR}/submission_lgb_refined.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ac9f035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Data ---\n",
      "Train Shape: (650000, 11)\n",
      "--- Processing Features ---\n",
      "\n",
      "--- Generating Ridge Meta-Feature ---\n",
      "\n",
      "--- Training Diverse LightGBM Ensemble ---\n",
      "GPU Detected.\n",
      "\n",
      "Fold 1 Params: Depth=8, Leaves=31\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's rmse: 8.78803\n",
      "[2000]\tvalid_0's rmse: 8.76142\n",
      "[3000]\tvalid_0's rmse: 8.7475\n",
      "[4000]\tvalid_0's rmse: 8.74058\n",
      "[5000]\tvalid_0's rmse: 8.73672\n",
      "[6000]\tvalid_0's rmse: 8.73429\n",
      "[7000]\tvalid_0's rmse: 8.73246\n",
      "Early stopping, best iteration is:\n",
      "[7133]\tvalid_0's rmse: 8.73218\n",
      "\n",
      "Fold 2 Params: Depth=12, Leaves=63\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's rmse: 8.83723\n",
      "[2000]\tvalid_0's rmse: 8.81251\n",
      "[3000]\tvalid_0's rmse: 8.80154\n",
      "[4000]\tvalid_0's rmse: 8.79488\n",
      "[5000]\tvalid_0's rmse: 8.79097\n",
      "[6000]\tvalid_0's rmse: 8.78775\n",
      "Early stopping, best iteration is:\n",
      "[6754]\tvalid_0's rmse: 8.78617\n",
      "\n",
      "Fold 3 Params: Depth=6, Leaves=15\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's rmse: 8.7999\n",
      "[2000]\tvalid_0's rmse: 8.77201\n",
      "[3000]\tvalid_0's rmse: 8.75729\n",
      "[4000]\tvalid_0's rmse: 8.74811\n",
      "[5000]\tvalid_0's rmse: 8.74177\n",
      "[6000]\tvalid_0's rmse: 8.7373\n",
      "[7000]\tvalid_0's rmse: 8.73374\n",
      "[8000]\tvalid_0's rmse: 8.73126\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[8000]\tvalid_0's rmse: 8.73126\n",
      "\n",
      "Fold 4 Params: Depth=10, Leaves=45\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's rmse: 8.81136\n",
      "[2000]\tvalid_0's rmse: 8.78661\n",
      "[3000]\tvalid_0's rmse: 8.77544\n",
      "[4000]\tvalid_0's rmse: 8.77018\n",
      "[5000]\tvalid_0's rmse: 8.76706\n",
      "[6000]\tvalid_0's rmse: 8.76465\n",
      "Early stopping, best iteration is:\n",
      "[6139]\tvalid_0's rmse: 8.76443\n",
      "\n",
      "Fold 5 Params: Depth=8, Leaves=31\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's rmse: 8.85583\n",
      "[2000]\tvalid_0's rmse: 8.83118\n",
      "[3000]\tvalid_0's rmse: 8.81809\n",
      "[4000]\tvalid_0's rmse: 8.81044\n",
      "[5000]\tvalid_0's rmse: 8.80557\n",
      "[6000]\tvalid_0's rmse: 8.80251\n",
      "[7000]\tvalid_0's rmse: 8.79968\n",
      "[8000]\tvalid_0's rmse: 8.79747\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[8000]\tvalid_0's rmse: 8.79747\n",
      "\n",
      "Overall CV RMSE: 8.76234\n",
      "Saved submission to '/rds/rds-lxu/ml_datasets/exam_score_predict/submission_diverse_ensemble.csv'\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# 1. IMPORTS & CONFIGURATION\n",
    "# ==========================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "import gc\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "RANDOM_STATE = 42\n",
    "N_SPLITS = 5 \n",
    "\n",
    "# --- HYPERPARAMETER ZOO (Diversity Strategy) ---\n",
    "# We will rotate through these 5 sets to force diversity\n",
    "PARAMS_ZOO = [\n",
    "    # Set 1: Baseline (Balanced)\n",
    "    {\n",
    "        'learning_rate': 0.015, 'num_leaves': 31, 'max_depth': 8, \n",
    "        'subsample': 0.8, 'colsample_bytree': 0.7, 'min_child_samples': 50,\n",
    "        'reg_alpha': 0.1, 'reg_lambda': 0.1\n",
    "    },\n",
    "    # Set 2: Deep & Aggressive (Catches complex patterns)\n",
    "    {\n",
    "        'learning_rate': 0.01, 'num_leaves': 63, 'max_depth': 12, \n",
    "        'subsample': 0.9, 'colsample_bytree': 0.8, 'min_child_samples': 30,\n",
    "        'reg_alpha': 0.05, 'reg_lambda': 0.05\n",
    "    },\n",
    "    # Set 3: Shallow & Conservative (Prevents overfitting)\n",
    "    {\n",
    "        'learning_rate': 0.02, 'num_leaves': 15, 'max_depth': 6, \n",
    "        'subsample': 0.7, 'colsample_bytree': 0.6, 'min_child_samples': 100,\n",
    "        'reg_alpha': 1.0, 'reg_lambda': 1.0\n",
    "    },\n",
    "    # Set 4: Wide Search (High colsample, low subsample)\n",
    "    {\n",
    "        'learning_rate': 0.015, 'num_leaves': 45, 'max_depth': 10, \n",
    "        'subsample': 0.6, 'colsample_bytree': 0.9, 'min_child_samples': 40,\n",
    "        'reg_alpha': 0.5, 'reg_lambda': 0.5\n",
    "    },\n",
    "    # Set 5: Regularization Heavy (L1/L2 focus)\n",
    "    {\n",
    "        'learning_rate': 0.012, 'num_leaves': 31, 'max_depth': 8, \n",
    "        'subsample': 0.8, 'colsample_bytree': 0.7, 'min_child_samples': 60,\n",
    "        'reg_alpha': 5.0, 'reg_lambda': 5.0\n",
    "    }\n",
    "]\n",
    "\n",
    "# Common Fixed Params\n",
    "FIXED_PARAMS = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'n_estimators': 8000,\n",
    "    'n_jobs': 8,               # Prevent CPU Explosion\n",
    "    'device': 'gpu',           # GPU Enabled\n",
    "    'gpu_platform_id': 0,\n",
    "    'gpu_device_id': 0,\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'verbosity': -1\n",
    "}\n",
    "\n",
    "# ==========================================================\n",
    "# 2. DATA LOADING & MEMORY OPTIMIZATION\n",
    "# ==========================================================\n",
    "print(\"--- Loading Data ---\")\n",
    "DATA_DIR = \"/rds/rds-lxu/ml_datasets/exam_score_predict\"\n",
    "\n",
    "df_train = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "df_test = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "original = pd.read_csv(f'{DATA_DIR}/Exam_Score_Prediction.csv')\n",
    "\n",
    "df_train = df_train.set_index('id')\n",
    "df_test = df_test.set_index('id')\n",
    "if 'student_id' in original.columns:\n",
    "    original = original.set_index('student_id')\n",
    "\n",
    "df_train = pd.concat([original, df_train], axis=0).reset_index(drop=True)\n",
    "\n",
    "y = df_train['exam_score'].values.astype(np.float32)\n",
    "df_train = df_train.drop(columns=['exam_score'])\n",
    "\n",
    "print(f\"Train Shape: {df_train.shape}\")\n",
    "gc.collect()\n",
    "\n",
    "# ==========================================================\n",
    "# 3. FEATURE ENGINEERING\n",
    "# ==========================================================\n",
    "def preprocess(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Ordinal\n",
    "    mappings = {\n",
    "        'sleep_quality': {'poor': 1, 'average': 2, 'good': 3},\n",
    "        'facility_rating': {'low': 1, 'medium': 2, 'high': 3},\n",
    "        'exam_difficulty': {'easy': 1, 'moderate': 2, 'hard': 3}\n",
    "    }\n",
    "    for col, mapping in mappings.items():\n",
    "        if col in df.columns:\n",
    "            df[f'{col}_score'] = df[col].map(mapping).fillna(2).astype(np.float32)\n",
    "\n",
    "    # Interactions\n",
    "    if 'study_hours' in df.columns and 'sleep_quality_score' in df.columns:\n",
    "        df['study_efficiency'] = df['study_hours'] * df['sleep_quality_score']\n",
    "    if 'class_attendance' in df.columns and 'facility_rating_score' in df.columns:\n",
    "        df['attendance_impact'] = df['class_attendance'] * df['facility_rating_score']\n",
    "    if 'study_hours' in df.columns:\n",
    "        df['study_hours_sq'] = df['study_hours'] ** 2\n",
    "\n",
    "    # Formula\n",
    "    is_good = (df['sleep_quality'] == 'good').astype(np.float32)\n",
    "    is_poor = (df['sleep_quality'] == 'poor').astype(np.float32)\n",
    "    is_coaching = (df['study_method'] == 'coaching').astype(np.float32)\n",
    "    is_high_fac = (df['facility_rating'] == 'high').astype(np.float32)\n",
    "    \n",
    "    df['formula'] = (6 * df['study_hours'] + \n",
    "                     0.35 * df['class_attendance'] + \n",
    "                     1.5 * df['sleep_hours'] +\n",
    "                     5 * is_good - 5 * is_poor +\n",
    "                     10 * is_coaching + \n",
    "                     4 * is_high_fac)\n",
    "\n",
    "    # Convert Categoricals to 'category'\n",
    "    cat_cols = ['gender', 'course', 'study_method', 'internet_access', \n",
    "                'sleep_quality', 'facility_rating', 'exam_difficulty']\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype('category')\n",
    "            \n",
    "    return df\n",
    "\n",
    "print(\"--- Processing Features ---\")\n",
    "X = preprocess(df_train)\n",
    "X_test = preprocess(df_test)\n",
    "\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Downcast\n",
    "for col in num_cols:\n",
    "    X[col] = X[col].astype(np.float32)\n",
    "    X_test[col] = X_test[col].astype(np.float32)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# ==========================================================\n",
    "# 4. LIGHTWEIGHT RIDGE STACKING\n",
    "# ==========================================================\n",
    "print(\"\\n--- Generating Ridge Meta-Feature ---\")\n",
    "scaler = StandardScaler()\n",
    "X_num_scaled = scaler.fit_transform(X[num_cols].fillna(0))\n",
    "X_test_num_scaled = scaler.transform(X_test[num_cols].fillna(0))\n",
    "\n",
    "kf_ridge = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "ridge_oof = np.zeros((X.shape[0],), dtype=np.float32)\n",
    "ridge_test_preds = np.zeros((X_test.shape[0], 5), dtype=np.float32)\n",
    "model_ridge = RidgeCV(alphas=[0.1, 1.0, 10.0])\n",
    "\n",
    "for fold, (idx_tr, idx_va) in enumerate(kf_ridge.split(X_num_scaled, y)):\n",
    "    model_ridge.fit(X_num_scaled[idx_tr], y[idx_tr])\n",
    "    ridge_oof[idx_va] = model_ridge.predict(X_num_scaled[idx_va])\n",
    "    ridge_test_preds[:, fold] = model_ridge.predict(X_test_num_scaled)\n",
    "\n",
    "X['ridge_pred'] = ridge_oof\n",
    "X_test['ridge_pred'] = ridge_test_preds.mean(axis=1)\n",
    "\n",
    "del X_num_scaled, X_test_num_scaled, ridge_test_preds\n",
    "gc.collect()\n",
    "\n",
    "# ==========================================================\n",
    "# 5. DIVERSE LIGHTGBM TRAINING (GPU)\n",
    "# ==========================================================\n",
    "print(\"\\n--- Training Diverse LightGBM Ensemble ---\")\n",
    "\n",
    "# Check GPU\n",
    "try:\n",
    "    lgb.train({'device': 'gpu'}, lgb.Dataset(np.array([[1]]), np.array([1])), num_boost_round=1)\n",
    "    print(\"GPU Detected.\")\n",
    "except:\n",
    "    print(\"WARNING: GPU init failed. Using CPU.\")\n",
    "    FIXED_PARAMS['device'] = 'cpu'\n",
    "\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "lgb_oof_preds = np.zeros(X.shape[0])\n",
    "lgb_test_preds = np.zeros((X_test.shape[0], N_SPLITS))\n",
    "\n",
    "for fold, (idx_tr, idx_va) in enumerate(kf.split(X, y)):\n",
    "    X_tr, y_tr = X.iloc[idx_tr], y[idx_tr]\n",
    "    X_va, y_va = X.iloc[idx_va], y[idx_va]\n",
    "    \n",
    "    # --- SELECT DIVERSE PARAMS FOR THIS FOLD ---\n",
    "    current_params = {**FIXED_PARAMS, **PARAMS_ZOO[fold % len(PARAMS_ZOO)]}\n",
    "    print(f\"\\nFold {fold+1} Params: Depth={current_params['max_depth']}, Leaves={current_params['num_leaves']}\")\n",
    "    \n",
    "    model = lgb.LGBMRegressor(**current_params)\n",
    "    \n",
    "    callbacks = [\n",
    "        lgb.early_stopping(stopping_rounds=100, verbose=True),\n",
    "        lgb.log_evaluation(period=1000)\n",
    "    ]\n",
    "    \n",
    "    model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_va, y_va)],\n",
    "        eval_metric='rmse',\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    lgb_oof_preds[idx_va] = model.predict(X_va)\n",
    "    lgb_test_preds[:, fold] = model.predict(X_test)\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "# ==========================================================\n",
    "# 6. SUBMISSION\n",
    "# ==========================================================\n",
    "final_rmse = np.sqrt(mean_squared_error(y, lgb_oof_preds))\n",
    "print(f\"\\nOverall CV RMSE: {final_rmse:.5f}\")\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': df_test.index,\n",
    "    'exam_score': np.clip(lgb_test_preds.mean(axis=1), 0, 100)\n",
    "})\n",
    "\n",
    "submission.to_csv(f'{DATA_DIR}/submission_diverse_ensemble.csv', index=False)\n",
    "print(f\"Saved submission to '{DATA_DIR}/submission_diverse_ensemble.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cmbagent_env)",
   "language": "python",
   "name": "cmbagent_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
